{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import pyro\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Scripts')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from Scripts.new_sc_model import open_csv\n",
    "from Scripts.new_sc_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = { \"bubble\": [\"Fig4\"],\n",
    "#              \"four_waystrandexchange\": [\"Table5.2\"],\n",
    "#              \"hairpin\" : [\"Fig4_0\", \"Fig4_1\", \"Fig6_0\", \"Fig6_1\"], \n",
    "#              \"hairpin1\" : [\"Fig3_T_0\", \"Fig3_T_1\"],\n",
    "#              \"hairpin4\" : [\"Table1_0\", \"Table1_1\"],\n",
    "#              \"helix\" : [\"Fig6_0\", \"Fig6_1\"],\n",
    "#              \"helix1\" : [\"Fig6a\"],\n",
    "#              \"three_waystranddisplacement\" : [\"Fig3b\"], \n",
    "#              \"three_waystranddisplacement1\" : [\"Fig6b\"]\n",
    "# }\n",
    "\n",
    "datasets = { \"hairpin\" : [\"Fig4_0\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_theta_to_rate(theta, datasets, kinetic_model=\"ARRHENIUS\", stochastic_conditionning=False):\n",
    "    \n",
    "    # PATH = '/Users/aliseyfi/Documents/UBC/Probabilistic-Programming/Probabilistic-Programming/Project/'\n",
    "    PATH = \"C:/Users/jlovr/CS532-project/Probabilistic-Programming/Project/\"\n",
    "    predicted_log_10_rates, real_log_10_rates, errors = [], [], []\n",
    "    for reaction_type in datasets:\n",
    "            if reaction_type == \"bubble\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_AltanBonnet(row, theta, file, reaction_id, str(row), \"Altanbonnet\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"four_waystrandexchange\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_DabbyThesis(row, theta, file, reaction_id, str(row), \"Dabby\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"hairpin\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    _zip = bool(int(reaction_dataset[-1]))\n",
    "                    j = reaction_dataset[-3]\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        if row <=10 :\n",
    "                            predicted_log_10_rate, real_log_10_rate, sq_error = estimate_Bonnet(row, theta, _zip, file, reaction_id, str(row), \"Bonnet\"+j, kinetic_model, stochastic_conditionning)\n",
    "                            predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                            real_log_10_rates.append(real_log_10_rate)\n",
    "                            errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"hairpin1\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    _zip = bool(int(reaction_dataset[-1]))\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_BonnetThesis(row, theta, _zip, file, reaction_id, str(row), \"GoddardT\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"hairpin4\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    _zip = bool(int(reaction_dataset[-1]))\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_Kim(row, theta, _zip, file, reaction_id, str(row), \"Kim\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"helix\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    _zip = bool(int(reaction_dataset[-1]))\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_Morrison(row, theta, _zip, file, reaction_id, str(row), \"Morrison\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"helix1\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    _zip = False\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_ReynaldoDissociate(row, theta, _zip, file, reaction_id, str(row), \"ReynaldoDissociate\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"three_waystranddisplacement\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_Zhang(row, theta, file, reaction_id, str(row), \"Zhang\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            if reaction_type == \"three_waystranddisplacement1\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        predicted_log_10_rate, real_log_10_rate, sq_error = estimate_ReyanldoSequential(row, theta, file, reaction_id, str(row), \"ReynaldoSequential\", kinetic_model)\n",
    "                        predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                        real_log_10_rates.append(real_log_10_rate)\n",
    "                        errors.append(sq_error)\n",
    "                        row+=1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return predicted_log_10_rates, real_log_10_rates, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling without Pyro (no stochastic conditionning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESS(W):\n",
    "    return 1/sum([wi**2 for wi in W])\n",
    "\n",
    "def weighted_avg(X, weights):\n",
    "    return (weights.dot(X)) / weights.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "mean [10.31096554  3.13765764 11.79217529  1.95416665 10.39432526  1.12080562\n",
      " 12.4910717   3.87731171 13.10680103  3.29378676 10.79393673  1.94823694\n",
      " 12.42702579  3.37145209  1.09886587]\n",
      "variance [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "ess 1.0\n"
     ]
    }
   ],
   "source": [
    "theta_dim = 15\n",
    "theta_mean = [13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3,   0.0402 ]\n",
    "full_samples,full_logWs,full_ESS,full_means,full_vars = [],[],[],[],[]\n",
    "\n",
    "\n",
    "for n_samples in [1]:\n",
    "    samples,logWs = [],[]\n",
    "    for i in range(n_samples):\n",
    "        theta = torch.distributions.MultivariateNormal(torch.tensor(theta_mean), torch.eye(theta_dim)).sample()\n",
    "\n",
    "        ks, reals, errors = from_theta_to_rate(theta, datasets, stochastic_conditionning=False)\n",
    "        loglik = 0\n",
    "        print(len(ks))\n",
    "\n",
    "        for ind, k in enumerate(ks):\n",
    "            error = errors[ind]\n",
    "            \n",
    "            # synthetic likelihood\n",
    "            loglik += torch.distributions.Normal(torch.tensor(0), torch.tensor(1)).log_prob(torch.tensor(error))\n",
    "        \n",
    "        samples.append(theta)\n",
    "        logWs.append(loglik)\n",
    "\n",
    "    W = np.exp([logwi - max(logWs) for logwi in logWs])\n",
    "    W = W/sum(W)\n",
    "    ess = ESS(W)\n",
    "\n",
    "    for n in range(n_samples):\n",
    "        samples[n] = [float(x) for x in samples[n]]\n",
    "\n",
    "    means = weighted_avg(samples, W)\n",
    "    vars = weighted_avg((samples - means)**2, W)\n",
    "\n",
    "    full_samples.append(samples)\n",
    "    full_logWs.append(logWs)\n",
    "    full_ESS.append(ess)\n",
    "    full_means.append(means)\n",
    "    full_vars.append(vars)\n",
    "\n",
    "    print(\"mean\", means)\n",
    "    print(\"variance\", vars)\n",
    "    print(\"ess\", ess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n"
     ]
    }
   ],
   "source": [
    "theta_dim = 15\n",
    "theta_mean = [13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3,   0.0402 ]\n",
    "full_samples,full_logWs,full_ESS,full_means,full_vars = [],[],[],[],[]\n",
    "\n",
    "\n",
    "for n_samples in [10]:\n",
    "    samples,logWs = [],[]\n",
    "    for i in range(n_samples):\n",
    "        theta = torch.distributions.MultivariateNormal(torch.tensor(theta_mean), torch.eye(theta_dim)).sample()\n",
    "\n",
    "        ks, reals, errors = from_theta_to_rate(theta, datasets, stochastic_conditionning=True)\n",
    "        loglik = 0\n",
    "\n",
    "        for ind, k in enumerate(ks):\n",
    "            error = errors[ind]\n",
    "            \n",
    "            # synthetic likelihood\n",
    "            # loglik += torch.distributions.Normal(torch.tensor(0), torch.tensor(1)).log_prob(torch.tensor(error))\n",
    "            loglik += np.log(error)\n",
    "\n",
    "        \n",
    "        samples.append(theta)\n",
    "        logWs.append(loglik)\n",
    "\n",
    "    W = np.exp([logwi - max(logWs) for logwi in logWs])\n",
    "    W = W/sum(W)\n",
    "    ess = ESS(W)\n",
    "\n",
    "    for n in range(n_samples):\n",
    "        samples[n] = [float(x) for x in samples[n]]\n",
    "\n",
    "    means = weighted_avg(samples, W)\n",
    "    vars = weighted_avg((samples - means)**2, W)\n",
    "\n",
    "    full_samples.append(samples)\n",
    "    full_logWs.append(logWs)\n",
    "    full_ESS.append(ess)\n",
    "    full_means.append(means)\n",
    "    full_vars.append(vars)\n",
    "\n",
    "    print(\"mean\", means)\n",
    "    print(\"variance\", vars)\n",
    "    print(\"ess\", ess)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ac34c6cd7eeed34b8ef5a26d96fb33c83d96247b1e5d7492eca8d43f8ea1173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
