{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import pyro\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Scripts')\n",
    "\n",
    "from Scripts.new_sc_model import open_csv\n",
    "from Scripts.new_sc_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = { \"bubble\": [\"Fig4\"],\n",
    "#              \"four_waystrandexchange\": [\"Table5.2\"],\n",
    "#              \"hairpin\" : [\"Fig4_0\", \"Fig4_1\", \"Fig6_0\", \"Fig6_1\"], \n",
    "#              \"hairpin1\" : [\"Fig3_T_0\", \"Fig3_T_1\"],\n",
    "#              \"hairpin4\" : [\"Table1_0\", \"Table1_1\"],\n",
    "#              \"helix\" : [\"Fig6_0\", \"Fig6_1\"],\n",
    "#              \"helix1\" : [\"Fig6a\"],\n",
    "#              \"three_waystranddisplacement\" : [\"Fig3b\"], \n",
    "#              \"three_waystranddisplacement1\" : [\"Fig6b\"]\n",
    "# }\n",
    "\n",
    "datasets = { \"hairpin\" : [\"Fig4_0\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_theta_to_rate(theta, datasets, kinetic_model=\"ARRHENIUS\", stochastic_conditionning=False):\n",
    "    \n",
    "    # PATH = '/Users/aliseyfi/Documents/UBC/Probabilistic-Programming/Probabilistic-Programming/Project/'\n",
    "    PATH = \"C:/Users/jlovr/CS532-project/Probabilistic-Programming/Project/\"\n",
    "    predicted_log_10_rates, real_log_10_rates, errors = [], [], []\n",
    "    for reaction_type in datasets:\n",
    "            if reaction_type == \"hairpin\":\n",
    "                for reaction_dataset in datasets[reaction_type]:\n",
    "                    _zip = bool(int(reaction_dataset[-1]))\n",
    "                    j = reaction_dataset[-3]\n",
    "                    reaction_id = \"/\" + reaction_type + \"/\" + reaction_dataset\n",
    "                    document_name = PATH + \"/dataset\" + reaction_id + \".csv\"\n",
    "                    file =  open_csv(document_name)\n",
    "                    row = 1\n",
    "                    while row < len(file) and file[row][0] != '' :\n",
    "                        if row <= 10:\n",
    "                            predicted_log_10_rate, real_log_10_rate, error = estimate_Bonnet(row, theta, _zip, file, reaction_id, str(row), \"Bonnet\"+j, kinetic_model, stochastic_conditionning)\n",
    "                            predicted_log_10_rates.append(predicted_log_10_rate)\n",
    "                            real_log_10_rates.append(real_log_10_rate)\n",
    "                            errors.append(error)\n",
    "                        row+=1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return predicted_log_10_rates, real_log_10_rates, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "\n",
    "    theta_dim = 15\n",
    "\n",
    "    # priors\n",
    "    # theta_mean = [13.0580, 5, 17.0580, 5,  10.0580, 1, 1.0580, -2,  13.0580, 1, 5.0580, 0,  4.0580, -2,   0.0402 ]\n",
    "    theta_mean = [13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3,   0.0402 ]\n",
    "    theta = pyro.sample('theta', pyro.distributions.Normal(torch.tensor(theta_mean), torch.ones(theta_dim)))\n",
    "\n",
    "    # likelihood\n",
    "    ks, reals, errors = from_theta_to_rate(theta, data, stochastic_conditionning=True)\n",
    "    for ind, k in enumerate(ks):\n",
    "        # error = abs(k-reals[ind])\n",
    "        error = errors[ind]\n",
    "        # pyro.sample('obs_'+str(ind), pyro.distributions.Exponential(1), obs=torch.tensor(reals[ind]))\n",
    "        # pyro.sample('obs_'+str(ind), pyro.distributions.Normal(k, 1), obs=torch.tensor(reals[ind]))\n",
    "        # pyro.sample('obs_'+str(ind), pyro.distributions.Beta(1, 3), obs=torch.tensor(reals[ind]))\n",
    "        pyro.sample('obs_'+str(ind), pyro.distributions.Normal(0, 1), obs=torch.tensor(error))\n",
    "\n",
    "        # do a test to make sure abs(k-reals[ind]) under N(0,1) is the same as reals[ind] under Normal(k,1)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(data):\n",
    "\n",
    "    theta_dim = 15\n",
    "\n",
    "    # priors\n",
    "    # theta_mean = [13.0580, 5, 17.0580, 5,  10.0580, 1, 1.0580, -2,  13.0580, 1, 5.0580, 0,  4.0580, -2,   0.0402 ]\n",
    "    theta_mean = [13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3,   0.0402 ]\n",
    "    theta = pyro.sample('theta', pyro.distributions.Normal(torch.tensor(theta_mean), torch.ones(theta_dim)))\n",
    "\n",
    "    # likelihood\n",
    "    ks, reals, errors = from_theta_to_rate(theta, data, stochastic_conditionning=False)\n",
    "    for ind, k in enumerate(ks):\n",
    "        # error = abs(k-reals[ind])\n",
    "        error = errors[ind]*errors[ind]\n",
    "        # pyro.sample('obs_'+str(ind), pyro.distributions.Exponential(1), obs=torch.tensor(reals[ind]))\n",
    "        # pyro.sample('obs_'+str(ind), pyro.distributions.Normal(k, 1), obs=torch.tensor(reals[ind]))\n",
    "        # pyro.sample('obs_'+str(ind), pyro.distributions.Beta(1, 3), obs=torch.tensor(reals[ind]))\n",
    "        pyro.sample('obs_'+str(ind), pyro.distributions.Normal(0, 1), obs=torch.tensor(error))\n",
    "\n",
    "        # do a test to make sure abs(k-reals[ind]) under N(0,1) is the same as reals[ind] under Normal(k,1)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling - original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(0)\n",
    "# inference with importance sampling\n",
    "importance = pyro.infer.Importance(model2, guide=None, num_samples=n_samples)\n",
    "\n",
    "print(\"doing importance sampling...\")\n",
    "emp_marginal = pyro.infer.EmpiricalMarginal(importance.run(datasets))\n",
    "\n",
    "posterior_mean = emp_marginal.mean\n",
    "posterior_std_dev = emp_marginal.variance.sqrt()\n",
    "ess = importance.get_ESS()\n",
    "\n",
    "\n",
    "# report results\n",
    "print(posterior_mean)\n",
    "print(posterior_std_dev)\n",
    "print(\"ess\", ess)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling - with stochastic conditionning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESS(W):\n",
    "    summ = np.sum(W)\n",
    "    W = [wi/summ for wi in W]\n",
    "    return 1/np.sum([w**2 for w in W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n",
      "getting score\n"
     ]
    }
   ],
   "source": [
    "theta_dim = 15\n",
    "n_samples = 100\n",
    "\n",
    "samples,logWs = [],[]\n",
    "theta_mean = [13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3, 13.0580, 3,  13.0580, 3,   0.0402 ]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    theta = pyro.sample('theta', pyro.distributions.Normal(torch.tensor(theta_mean), torch.ones(theta_dim)))\n",
    "\n",
    "    # likelihood\n",
    "    ks, reals, errors = from_theta_to_rate(theta, datasets, stochastic_conditionning=True)\n",
    "    loglik = 0\n",
    "\n",
    "    for ind, k in enumerate(ks):\n",
    "        # error = errors[ind]\n",
    "        error = errors[ind]\n",
    "        try:\n",
    "            loglik += np.log(1 - error)\n",
    "        except:\n",
    "            loglik = -np.inf\n",
    "    \n",
    "    samples.append(theta)\n",
    "    logWs.append(loglik)\n",
    "\n",
    "print(samples)\n",
    "print(logWs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [11.6647625   3.59745693 12.73785877  3.43894148 13.29020977  2.80181003\n",
      " 14.36070633  1.94243133 13.43860149  2.38264942 13.17202282  4.81843758\n",
      " 11.63335896  3.9693923  -1.29722977]\n",
      "variance [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.97215226e-31\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "ess 1.0\n"
     ]
    }
   ],
   "source": [
    "def weighted_avg(X, weights):\n",
    "    return (weights.dot(X)) / weights.sum()\n",
    "\n",
    "for n in range(n_samples):\n",
    "    samples[n] = np.array(samples[n], dtype=float)\n",
    "\n",
    "W = np.exp(logWs)\n",
    "means = weighted_avg(samples, W)\n",
    "vars = weighted_avg((samples - means)**2, W)\n",
    "\n",
    "print(\"mean\", means)\n",
    "print(\"variance\", vars)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ac34c6cd7eeed34b8ef5a26d96fb33c83d96247b1e5d7492eca8d43f8ea1173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
